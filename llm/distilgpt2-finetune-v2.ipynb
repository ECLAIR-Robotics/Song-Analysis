{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c78a7fb-edce-4a67-917a-f0099a9303d8",
   "metadata": {},
   "source": [
    "# distilGPT2 Fine-Tuning v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe50c734-113a-416e-a590-b8a5f6cbb3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "ver = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b86845-b402-41ed-9572-764d6a99b09c",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e94eee7e-3c43-43e4-bc11-58efce82e4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>lastfm_url</th>\n",
       "      <th>track</th>\n",
       "      <th>artist</th>\n",
       "      <th>seeds</th>\n",
       "      <th>number_of_emotion_tags</th>\n",
       "      <th>valence_tags</th>\n",
       "      <th>arousal_tags</th>\n",
       "      <th>dominance_tags</th>\n",
       "      <th>mbid</th>\n",
       "      <th>spotify_id</th>\n",
       "      <th>genre</th>\n",
       "      <th>Lyric</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.last.fm/music/metallica/_/st.%2banger</td>\n",
       "      <td>St. Anger</td>\n",
       "      <td>Metallica</td>\n",
       "      <td>['aggressive']</td>\n",
       "      <td>8</td>\n",
       "      <td>3.710000</td>\n",
       "      <td>5.833000</td>\n",
       "      <td>5.427250</td>\n",
       "      <td>727a2529-7ee8-4860-aef6-7959884895cb</td>\n",
       "      <td>3fOc9x06lKJBhz435mInlH</td>\n",
       "      <td>metal</td>\n",
       "      <td>Saint Anger 'round my neck\\nSaint Anger 'round...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>https://www.last.fm/music/m.i.a./_/bamboo%2bbanga</td>\n",
       "      <td>Bamboo Banga</td>\n",
       "      <td>M.I.A.</td>\n",
       "      <td>['aggressive', 'fun', 'sexy', 'energetic']</td>\n",
       "      <td>13</td>\n",
       "      <td>6.555071</td>\n",
       "      <td>5.537214</td>\n",
       "      <td>5.691357</td>\n",
       "      <td>99dd2c8c-e7c1-413e-8ea4-4497a00ffa18</td>\n",
       "      <td>6tqFC1DIOphJkCwrjVzPmg</td>\n",
       "      <td>hip-hop</td>\n",
       "      <td>Road runner, road runner\\nGoing hundred mile p...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>https://www.last.fm/music/drowning%2bpool/_/st...</td>\n",
       "      <td>Step Up</td>\n",
       "      <td>Drowning Pool</td>\n",
       "      <td>['aggressive']</td>\n",
       "      <td>9</td>\n",
       "      <td>2.971389</td>\n",
       "      <td>5.537500</td>\n",
       "      <td>4.726389</td>\n",
       "      <td>49e7b4d2-3772-4301-ba25-3cc46ceb342e</td>\n",
       "      <td>4Q1w4Ryyi8KNxxaFlOQClK</td>\n",
       "      <td>metal</td>\n",
       "      <td>Come!\\n\\nIf our own lives aren’t directly affe...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>https://www.last.fm/music/kanye%2bwest/_/feedback</td>\n",
       "      <td>Feedback</td>\n",
       "      <td>Kanye West</td>\n",
       "      <td>['aggressive']</td>\n",
       "      <td>1</td>\n",
       "      <td>3.080000</td>\n",
       "      <td>5.870000</td>\n",
       "      <td>5.490000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49fT6owWuknekShh9utsjv</td>\n",
       "      <td>hip-hop</td>\n",
       "      <td>Ayy, y'all heard about the good news?\\nY'all s...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>https://www.last.fm/music/deftones/_/7%2bwords</td>\n",
       "      <td>7 Words</td>\n",
       "      <td>Deftones</td>\n",
       "      <td>['aggressive', 'angry']</td>\n",
       "      <td>10</td>\n",
       "      <td>3.807121</td>\n",
       "      <td>5.473939</td>\n",
       "      <td>4.729091</td>\n",
       "      <td>1a826083-5585-445f-a708-415dc90aa050</td>\n",
       "      <td>6DoXuH326aAYEN8CnlLmhP</td>\n",
       "      <td>nu metal</td>\n",
       "      <td>I'll never be the same, breaking decency\\nDon'...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16792</th>\n",
       "      <td>229432</td>\n",
       "      <td>https://www.last.fm/music/noblegases/_/xenon</td>\n",
       "      <td>Xenon</td>\n",
       "      <td>NobleGases</td>\n",
       "      <td>['noble']</td>\n",
       "      <td>2</td>\n",
       "      <td>6.160000</td>\n",
       "      <td>3.695000</td>\n",
       "      <td>6.130000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1AePjgLLtzF0abbfcgYdLI</td>\n",
       "      <td>chill</td>\n",
       "      <td>You're floating out astray\\nThis cold and life...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16793</th>\n",
       "      <td>229435</td>\n",
       "      <td>https://www.last.fm/music/kurt%2bvile/_/wild%2...</td>\n",
       "      <td>Wild Imagination</td>\n",
       "      <td>Kurt Vile</td>\n",
       "      <td>['transparent']</td>\n",
       "      <td>2</td>\n",
       "      <td>6.925000</td>\n",
       "      <td>4.975000</td>\n",
       "      <td>6.190000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1Gn0oYQiQHp7KF4DcR2g4t</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm looking at you\\nBut It's only a picture so...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16794</th>\n",
       "      <td>229436</td>\n",
       "      <td>https://www.last.fm/music/portugal.%2bthe%2bma...</td>\n",
       "      <td>Oh Lord</td>\n",
       "      <td>Portugal. The Man</td>\n",
       "      <td>['transparent']</td>\n",
       "      <td>1</td>\n",
       "      <td>5.370000</td>\n",
       "      <td>3.450000</td>\n",
       "      <td>5.330000</td>\n",
       "      <td>7ea228f9-16d0-474d-8c51-5a1a9810ddde</td>\n",
       "      <td>6YG8cjbrjhDhlYMiQnibUD</td>\n",
       "      <td>indie</td>\n",
       "      <td>\\n\\n\\nWhere do I fit in\\nI am waiting here for...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16795</th>\n",
       "      <td>229443</td>\n",
       "      <td>https://www.last.fm/music/porcelain%2band%2bth...</td>\n",
       "      <td>Transparent</td>\n",
       "      <td>Porcelain and The Tramps</td>\n",
       "      <td>['transparent']</td>\n",
       "      <td>3</td>\n",
       "      <td>6.613333</td>\n",
       "      <td>4.633333</td>\n",
       "      <td>5.773333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>industrial</td>\n",
       "      <td>Wish I were transparent\\nYou could see right t...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16796</th>\n",
       "      <td>229473</td>\n",
       "      <td>https://www.last.fm/music/daniel%2blanois/_/lo...</td>\n",
       "      <td>Lovechild</td>\n",
       "      <td>Daniel Lanois</td>\n",
       "      <td>['transparent']</td>\n",
       "      <td>2</td>\n",
       "      <td>6.685000</td>\n",
       "      <td>4.405000</td>\n",
       "      <td>5.625000</td>\n",
       "      <td>16c3d394-c4d4-4dc2-bbf1-b2bef3ac861c</td>\n",
       "      <td>4fVObxldDzxxRD6a5Eth9s</td>\n",
       "      <td>indie</td>\n",
       "      <td>I CAUGHT HER STARING, A PSYCHEDELIC DANCER,\\nG...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16797 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                         lastfm_url  \\\n",
       "0               1  https://www.last.fm/music/metallica/_/st.%2banger   \n",
       "1               3  https://www.last.fm/music/m.i.a./_/bamboo%2bbanga   \n",
       "2               5  https://www.last.fm/music/drowning%2bpool/_/st...   \n",
       "3              11  https://www.last.fm/music/kanye%2bwest/_/feedback   \n",
       "4              13     https://www.last.fm/music/deftones/_/7%2bwords   \n",
       "...           ...                                                ...   \n",
       "16792      229432       https://www.last.fm/music/noblegases/_/xenon   \n",
       "16793      229435  https://www.last.fm/music/kurt%2bvile/_/wild%2...   \n",
       "16794      229436  https://www.last.fm/music/portugal.%2bthe%2bma...   \n",
       "16795      229443  https://www.last.fm/music/porcelain%2band%2bth...   \n",
       "16796      229473  https://www.last.fm/music/daniel%2blanois/_/lo...   \n",
       "\n",
       "                  track                    artist  \\\n",
       "0             St. Anger                 Metallica   \n",
       "1          Bamboo Banga                    M.I.A.   \n",
       "2               Step Up             Drowning Pool   \n",
       "3              Feedback                Kanye West   \n",
       "4               7 Words                  Deftones   \n",
       "...                 ...                       ...   \n",
       "16792             Xenon                NobleGases   \n",
       "16793  Wild Imagination                 Kurt Vile   \n",
       "16794           Oh Lord         Portugal. The Man   \n",
       "16795       Transparent  Porcelain and The Tramps   \n",
       "16796         Lovechild             Daniel Lanois   \n",
       "\n",
       "                                            seeds  number_of_emotion_tags  \\\n",
       "0                                  ['aggressive']                       8   \n",
       "1      ['aggressive', 'fun', 'sexy', 'energetic']                      13   \n",
       "2                                  ['aggressive']                       9   \n",
       "3                                  ['aggressive']                       1   \n",
       "4                         ['aggressive', 'angry']                      10   \n",
       "...                                           ...                     ...   \n",
       "16792                                   ['noble']                       2   \n",
       "16793                             ['transparent']                       2   \n",
       "16794                             ['transparent']                       1   \n",
       "16795                             ['transparent']                       3   \n",
       "16796                             ['transparent']                       2   \n",
       "\n",
       "       valence_tags  arousal_tags  dominance_tags  \\\n",
       "0          3.710000      5.833000        5.427250   \n",
       "1          6.555071      5.537214        5.691357   \n",
       "2          2.971389      5.537500        4.726389   \n",
       "3          3.080000      5.870000        5.490000   \n",
       "4          3.807121      5.473939        4.729091   \n",
       "...             ...           ...             ...   \n",
       "16792      6.160000      3.695000        6.130000   \n",
       "16793      6.925000      4.975000        6.190000   \n",
       "16794      5.370000      3.450000        5.330000   \n",
       "16795      6.613333      4.633333        5.773333   \n",
       "16796      6.685000      4.405000        5.625000   \n",
       "\n",
       "                                       mbid              spotify_id  \\\n",
       "0      727a2529-7ee8-4860-aef6-7959884895cb  3fOc9x06lKJBhz435mInlH   \n",
       "1      99dd2c8c-e7c1-413e-8ea4-4497a00ffa18  6tqFC1DIOphJkCwrjVzPmg   \n",
       "2      49e7b4d2-3772-4301-ba25-3cc46ceb342e  4Q1w4Ryyi8KNxxaFlOQClK   \n",
       "3                                       NaN  49fT6owWuknekShh9utsjv   \n",
       "4      1a826083-5585-445f-a708-415dc90aa050  6DoXuH326aAYEN8CnlLmhP   \n",
       "...                                     ...                     ...   \n",
       "16792                                   NaN  1AePjgLLtzF0abbfcgYdLI   \n",
       "16793                                   NaN  1Gn0oYQiQHp7KF4DcR2g4t   \n",
       "16794  7ea228f9-16d0-474d-8c51-5a1a9810ddde  6YG8cjbrjhDhlYMiQnibUD   \n",
       "16795                                   NaN                     NaN   \n",
       "16796  16c3d394-c4d4-4dc2-bbf1-b2bef3ac861c  4fVObxldDzxxRD6a5Eth9s   \n",
       "\n",
       "            genre                                              Lyric language  \n",
       "0           metal  Saint Anger 'round my neck\\nSaint Anger 'round...       en  \n",
       "1         hip-hop  Road runner, road runner\\nGoing hundred mile p...       en  \n",
       "2           metal  Come!\\n\\nIf our own lives aren’t directly affe...       en  \n",
       "3         hip-hop  Ayy, y'all heard about the good news?\\nY'all s...       en  \n",
       "4        nu metal  I'll never be the same, breaking decency\\nDon'...       en  \n",
       "...           ...                                                ...      ...  \n",
       "16792       chill  You're floating out astray\\nThis cold and life...       en  \n",
       "16793         NaN  I'm looking at you\\nBut It's only a picture so...       en  \n",
       "16794       indie  \\n\\n\\nWhere do I fit in\\nI am waiting here for...       en  \n",
       "16795  industrial  Wish I were transparent\\nYou could see right t...       en  \n",
       "16796       indie  I CAUGHT HER STARING, A PSYCHEDELIC DANCER,\\nG...       en  \n",
       "\n",
       "[16797 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('../out.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a754827-5cdc-4eea-aa68-6daa6cc46fad",
   "metadata": {},
   "source": [
    "## Load Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c970d7d-e633-4735-ae3b-608500c9c0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method add_special_tokens in module transformers.tokenization_utils_base:\n",
      "\n",
      "add_special_tokens(special_tokens_dict: Dict[str, Union[str, tokenizers.AddedToken]], replace_additional_special_tokens=True) -> int method of transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast instance\n",
      "    Add a dictionary of special tokens (eos, pad, cls, etc.) to the encoder and link them to class attributes. If\n",
      "    special tokens are NOT in the vocabulary, they are added to it (indexed starting from the last index of the\n",
      "    current vocabulary).\n",
      "    \n",
      "    Note,None When adding new tokens to the vocabulary, you should make sure to also resize the token embedding\n",
      "    matrix of the model so that its embedding matrix matches the tokenizer.\n",
      "    \n",
      "    In order to do that, please use the [`~PreTrainedModel.resize_token_embeddings`] method.\n",
      "    \n",
      "    Using `add_special_tokens` will ensure your special tokens can be used in several ways:\n",
      "    \n",
      "    - Special tokens are carefully handled by the tokenizer (they are never split).\n",
      "    - You can easily refer to special tokens using tokenizer class attributes like `tokenizer.cls_token`. This\n",
      "      makes it easy to develop model-agnostic training and fine-tuning scripts.\n",
      "    \n",
      "    When possible, special tokens are already registered for provided pretrained models (for instance\n",
      "    [`BertTokenizer`] `cls_token` is already registered to be :obj*'[CLS]'* and XLM's one is also registered to be\n",
      "    `'</s>'`).\n",
      "    \n",
      "    Args:\n",
      "        special_tokens_dict (dictionary *str* to *str* or `tokenizers.AddedToken`):\n",
      "            Keys should be in the list of predefined special attributes: [`bos_token`, `eos_token`, `unk_token`,\n",
      "            `sep_token`, `pad_token`, `cls_token`, `mask_token`, `additional_special_tokens`].\n",
      "    \n",
      "            Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer\n",
      "            assign the index of the `unk_token` to them).\n",
      "        replace_additional_special_tokens (`bool`, *optional*,, defaults to `True`):\n",
      "            If `True`, the existing list of additional special tokens will be replaced by the one specified in\n",
      "            `special_tokens_dict`. Otherwise, `self._additional_special_tokens` is updated. In the former case, the\n",
      "            tokens will NOT be removed from the tokenizer's full vocabulary - they are only being flagged as\n",
      "            non-special tokens.\n",
      "    \n",
      "    Returns:\n",
      "        `int`: Number of tokens added to the vocabulary.\n",
      "    \n",
      "    Examples:\n",
      "    \n",
      "    ```python\n",
      "    # Let's see how to add a new classification token to GPT-2\n",
      "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
      "    model = GPT2Model.from_pretrained(\"gpt2\")\n",
      "    \n",
      "    special_tokens_dict = {\"cls_token\": \"<CLS>\"}\n",
      "    \n",
      "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
      "    print(\"We have added\", num_added_toks, \"tokens\")\n",
      "    # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\n",
      "    model.resize_token_embeddings(len(tokenizer))\n",
      "    \n",
      "    assert tokenizer.cls_token == \"<CLS>\"\n",
      "    ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tokenizer.add_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "35ff215e-053f-46c7-bd06-def419fba467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40f05ad-864c-40b6-834c-bfd62452fa94",
   "metadata": {},
   "source": [
    "## Preprocess Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb55f0c7-5eeb-4a6f-a59e-b59345014fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['seeds'] = data['seeds'].apply(lambda i: eval(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e499d65a-53a0-466d-9525-a1c38aafd88e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4778234208489611"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['seeds'].apply(len).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c266ae38-c603-49f4-a0f1-14c108269b8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocess Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "f25c794a-1de1-45f0-b653-cf8df71183aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 128\n",
    "buffer = 8\n",
    "question = 'What are the moods evoked by the following song excerpt?'\n",
    "\n",
    "def text_repr(lyrics: str, tags: list) -> list[str]:\n",
    "    tags = tags[:10]  # Trim the number of tags to prevent some errors\n",
    "    extra_tokens = len(tokenizer(f'Q: {question}\\n\\nA: {\", \".join(tags)}{tokenizer.eos_token}')['input_ids'])\n",
    "    this_chunk_size = chunk_size - extra_tokens\n",
    "    all_tokens = tokenizer(lyrics)['input_ids']\n",
    "    chunks = [all_tokens[i:i + this_chunk_size] for i in range(0, len(all_tokens), this_chunk_size)]\n",
    "    chunks = [f'Q: {question}\\n{tokenizer.decode(i)}\\nA: {\", \".join(tags)}{tokenizer.eos_token}' for i in chunks]\n",
    "    chunks = tokenizer(chunks, padding=True, truncation=True, max_length=chunk_size + buffer)['input_ids']\n",
    "    return chunks[:-1]  # Remove last element to avoid misaligned sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee76642-9a01-41c6-955d-afe19fbdb62d",
   "metadata": {},
   "source": [
    "This code creates a training dataset with all lyrics + tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "5a9c1bfc-0425-4eb2-9cf6-faa9ed2efa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    try:\n",
    "        text_repr(prompts[i], tags[i])\n",
    "    except:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "44691d69-9d4f-4964-a924-0acd54767fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 722\n",
    "chunks = text_repr(prompts.iloc[idx], tags.iloc[idx])\n",
    "# chunks\n",
    "# text_repr(prompts.iloc[idx], tags.iloc[idx])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "cb4598c9-923c-4ed3-b3bc-37bc60e2f147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7675ecc92a604e16849034f9158c86c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16797 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompts = data['Lyric']\n",
    "tags = data['seeds']\n",
    "\n",
    "tokenized_data_expanded = [text_repr(prompts.iloc[i], tags.iloc[i]) for i in tqdm(range(len(prompts)))]\n",
    "tokenized_data = []\n",
    "for i in tokenized_data_expanded:\n",
    "    tokenized_data += i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "15678282-50d7-4fa8-9680-17d0c4296ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = [i for i in tokenized_data if len(i) == chunk_size]  # Remove any data which still isn't the right size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6379d9d1-b4b9-4b8b-944b-5fbf8c51ff75",
   "metadata": {},
   "source": [
    "## Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "55ce43aa-6bc3-408a-bdf9-3f4ce2a732b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val = train_test_split(tokenized_data, test_size=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "cc1688d5-a69a-4dfa-8b32-e96e865099d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What are the moods evoked by the following song excerpt?\n",
      " na, na, na, na, na, na, na, na, na)\n",
      "Oh (na, na, na, na, na, na, na, na, na, na, na, na, na, na, na, na)\n",
      "Don't say you love me,\n",
      "You don't even know me, baby...........\n",
      "\n",
      "Baby, don't say you...\n",
      "(If you really want me then give me some time)\n",
      "Give me some time......\n",
      "\n",
      "(Chorus x3\n",
      "A: bright<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(x_train[23]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "0cf561ee-cfcf-4558-bbc2-0d35121652f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "3f6f5bf6-c0b0-410d-a193-91d8e706742f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='77235' max='77235' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [77235/77235 19:14:23, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.920100</td>\n",
       "      <td>2.845643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.847500</td>\n",
       "      <td>2.806225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.820100</td>\n",
       "      <td>2.782271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.754400</td>\n",
       "      <td>2.765009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.725600</td>\n",
       "      <td>2.752496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.704000</td>\n",
       "      <td>2.743229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.713300</td>\n",
       "      <td>2.739249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.665500</td>\n",
       "      <td>2.732555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.646800</td>\n",
       "      <td>2.729696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.635600</td>\n",
       "      <td>2.726301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.631400</td>\n",
       "      <td>2.724389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.603200</td>\n",
       "      <td>2.723296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.619300</td>\n",
       "      <td>2.720253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.613300</td>\n",
       "      <td>2.720037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.593200</td>\n",
       "      <td>2.720516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=77235, training_loss=2.7027885776326412, metrics={'train_runtime': 69264.2248, 'train_samples_per_second': 8.919, 'train_steps_per_second': 1.115, 'total_flos': 2.01778250121216e+16, 'train_loss': 2.7027885776326412, 'epoch': 15.0})"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"distilgpt2-finetuned-v{ver}\",\n",
    "    save_total_limit=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # eval_steps=10,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=15\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=x_train,\n",
    "    eval_dataset=x_val,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "e67a2399-c342-4994-922f-d93549040dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(f'v{ver}_final_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0012c750-d659-4dad-8959-1276081fea65",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "4a0de6d9-cf0c-4ab4-ad64-4127ec1910fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "text_generator = pipeline('text-generation', tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "042cb98a-b037-4fe6-ab75-fc9ccfa48469",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 128\n",
    "buffer = 8\n",
    "question = 'What are the moods evoked by the following song excerpt?'\n",
    "\n",
    "def process_lyrics(lyrics: str) -> list[str]:\n",
    "    extra_tokens = len(tokenizer(f'Q: {question}\\n\\nA: ')['input_ids'])\n",
    "    this_chunk_size = chunk_size - extra_tokens\n",
    "    all_tokens = tokenizer(lyrics)['input_ids']\n",
    "    chunks = [all_tokens[i:i + this_chunk_size] for i in range(0, len(all_tokens), this_chunk_size)]\n",
    "    chunks = [f'Q: {question}\\n{tokenizer.decode(i)}\\nA: ' for i in chunks]\n",
    "    chunks = tokenizer(chunks, padding=True, truncation=True, max_length=chunk_size + buffer)['input_ids']\n",
    "    return chunks[:-1]  # Remove last element to avoid misaligned sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "9512b8f8-d09b-48b3-8771-279936d788b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "LYRICS = \"\"\"\n",
    "Whores in this house\n",
    "There's some whores in this house\n",
    "There's some whores in this house\n",
    "There's some whores in this house (Hol' up)\n",
    "I said certified freak, seven days a week\n",
    "Wet-ass pussy, make that pullout game weak, woo (Ah)\n",
    "\n",
    "Yeah, yeah, yeah, yeah\n",
    "Yeah, you fuckin' with some wet-ass pussy\n",
    "Bring a bucket and a mop for this wet-ass pussy\n",
    "Give me everything you got for this wet-ass pussy\n",
    "\n",
    "Beat it up, nigga, catch a charge\n",
    "Extra large and extra hard\n",
    "Put this pussy right in your face\n",
    "Swipe your nose like a credit card\n",
    "Hop on top, I wanna ride\n",
    "I do a kegel while it's inside\n",
    "Spit in my mouth, look in my eyes\n",
    "This pussy is wet, come take a dive\n",
    "Tie me up like I'm surprised\n",
    "Let's roleplay, I'll wear a disguise\n",
    "I want you to park that big Mack truck right in this little garage\n",
    "Make it cream, make me scream\n",
    "Out in public, make a scene\n",
    "I don't cook, I don't clean\n",
    "But let me tell you how I got this ring (Ayy, ayy)\n",
    "\n",
    "Gobble me, swallow me, drip down the side of me (Yeah)\n",
    "Quick, jump out 'fore you let it get inside of me (Yeah)\n",
    "I tell him where to put it, never tell him where I'm 'bout to be (Huh)\n",
    "I'll run down on him 'fore I have a nigga runnin' me (Pow, pow, pow)\n",
    "Talk your shit, bite your lip (Yeah)\n",
    "Ask for a car while you ride that dick (While you ride that dick)\n",
    "You really ain't never gotta fuck him for a thang (Yeah)\n",
    "He already made his mind up 'fore he came (Ayy, ah)\n",
    "Now get your boots and your coat for this wet-ass pussy (Ah, ah, ah)\n",
    "He bought a phone just for pictures of this wet-ass pussy (Click, click, click)\n",
    "Paid my tuition just to kiss me on this wet-ass pussy (Mwah, mwah, mwah)\n",
    "Now make it rain if you wanna see some wet-ass pussy (Yeah, yeah)\n",
    "\n",
    "Look, I need a hard hitter, need a deep stroker\n",
    "Need a Henny drinker, need a weed smoker\n",
    "Not a garter snake, I need a king cobra\n",
    "With a hook in it, hope it lean over\n",
    "He got some money, then that's where I'm headed\n",
    "Pussy A1 just like his credit\n",
    "He got a beard, well, I'm tryna wet it\n",
    "I let him taste it, now he diabetic\n",
    "I don't wanna spit, I wanna gulp\n",
    "I wanna gag, I wanna choke\n",
    "I want you to touch that lil' dangly thing that swing in the back of my throat\n",
    "My head game is fire, punani Dasani\n",
    "It's goin' in dry and it's comin' out soggy\n",
    "I ride on that thing like the cops is behind me (Yeah, ah)\n",
    "I spit on his mic and now he tryna sign me, woo\n",
    "\n",
    "Your honor, I'm a freak bitch, handcuffs, leashes\n",
    "Switch my wig, make him feel like he cheatin'\n",
    "Put him on his knees, give him somethin' to believe in\n",
    "Never lost a fight, but I'm lookin' for a beatin' (Ah)\n",
    "In the food chain, I'm the one that eat ya\n",
    "If he ate my ass, he's a bottom-feeder\n",
    "Big D stand for big demeanor\n",
    "I could make ya bust before I ever meet ya\n",
    "If it don't hang, then he can't bang\n",
    "You can't hurt my feelings, but I like pain\n",
    "If he fuck me and ask \"Whose is it?\"\n",
    "When I ride the dick, I'ma spell my name, ah\n",
    "\n",
    "Yeah, yeah, yeah\n",
    "Yeah, you fuckin' with some wet-ass pussy\n",
    "Bring a bucket and a mop for this wet-ass pussy\n",
    "Give me everything you got for this wet-ass pussy\n",
    "Now from the top, make it drop, that's some wet-ass pussy\n",
    "Now get a bucket and a mop, that's some wet-ass pussy\n",
    "I'm talkin' wap, wap, wap, that's some wet-ass pussy\n",
    "Macaroni in a pot, that's some wet-ass pussy, huh\n",
    "\n",
    "There's some whores in this house\n",
    "There's some whores in this house\n",
    "There's some whores in this house\n",
    "There's some whores in this house\n",
    "There's some whores in this house\n",
    "There's some whores in this house\n",
    "There's some whores in this house\n",
    "There's some whores in this house\n",
    "There's some whores in this house\n",
    "There's some whores in this house\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "05caa612-19ca-4489-946e-846ea4e560e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_lyrics_chunks = process_lyrics(LYRICS)\n",
    "len(processed_lyrics_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "f48245de-7592-4833-a260-02bd3074b9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What are the moods evoked by the following song excerpt?\n",
      " pussy\n",
      "Give me everything you got for this wet-ass pussy\n",
      "\n",
      "Beat it up, nigga, catch a charge\n",
      "Extra large and extra hard\n",
      "Put this pussy right in your face\n",
      "Swipe your nose like a credit card\n",
      "Hop on top, I wanna ride\n",
      "I do a kegel while it's inside\n",
      "Spit in my mouth, look in my eyes\n",
      "This pussy is wet, come take a dive\n",
      "Tie me up like I'm surprised\n",
      "Let's roleplay, I'll wear a disguise\n",
      "\n",
      "A: ive sexual, sexy, light, quirky, calm\n"
     ]
    }
   ],
   "source": [
    "idx = 1\n",
    "print(text_generator(tokenizer.decode(processed_lyrics_chunks[idx]), \n",
    "                     max_length=len(processed_lyrics_chunks[idx]) + 10)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59b2dcf-00c3-4bd5-8c7d-aafa2ba32183",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
