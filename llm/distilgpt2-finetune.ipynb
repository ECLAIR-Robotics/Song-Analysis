{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c78a7fb-edce-4a67-917a-f0099a9303d8",
   "metadata": {},
   "source": [
    "# distilGPT2 Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "fe50c734-113a-416e-a590-b8a5f6cbb3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "ver = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b86845-b402-41ed-9572-764d6a99b09c",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e94eee7e-3c43-43e4-bc11-58efce82e4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>lastfm_url</th>\n",
       "      <th>track</th>\n",
       "      <th>artist</th>\n",
       "      <th>seeds</th>\n",
       "      <th>number_of_emotion_tags</th>\n",
       "      <th>valence_tags</th>\n",
       "      <th>arousal_tags</th>\n",
       "      <th>dominance_tags</th>\n",
       "      <th>mbid</th>\n",
       "      <th>spotify_id</th>\n",
       "      <th>genre</th>\n",
       "      <th>Lyric</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.last.fm/music/metallica/_/st.%2banger</td>\n",
       "      <td>St. Anger</td>\n",
       "      <td>Metallica</td>\n",
       "      <td>['aggressive']</td>\n",
       "      <td>8</td>\n",
       "      <td>3.710000</td>\n",
       "      <td>5.833000</td>\n",
       "      <td>5.427250</td>\n",
       "      <td>727a2529-7ee8-4860-aef6-7959884895cb</td>\n",
       "      <td>3fOc9x06lKJBhz435mInlH</td>\n",
       "      <td>metal</td>\n",
       "      <td>Saint Anger 'round my neck\\nSaint Anger 'round...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>https://www.last.fm/music/m.i.a./_/bamboo%2bbanga</td>\n",
       "      <td>Bamboo Banga</td>\n",
       "      <td>M.I.A.</td>\n",
       "      <td>['aggressive', 'fun', 'sexy', 'energetic']</td>\n",
       "      <td>13</td>\n",
       "      <td>6.555071</td>\n",
       "      <td>5.537214</td>\n",
       "      <td>5.691357</td>\n",
       "      <td>99dd2c8c-e7c1-413e-8ea4-4497a00ffa18</td>\n",
       "      <td>6tqFC1DIOphJkCwrjVzPmg</td>\n",
       "      <td>hip-hop</td>\n",
       "      <td>Road runner, road runner\\nGoing hundred mile p...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>https://www.last.fm/music/drowning%2bpool/_/st...</td>\n",
       "      <td>Step Up</td>\n",
       "      <td>Drowning Pool</td>\n",
       "      <td>['aggressive']</td>\n",
       "      <td>9</td>\n",
       "      <td>2.971389</td>\n",
       "      <td>5.537500</td>\n",
       "      <td>4.726389</td>\n",
       "      <td>49e7b4d2-3772-4301-ba25-3cc46ceb342e</td>\n",
       "      <td>4Q1w4Ryyi8KNxxaFlOQClK</td>\n",
       "      <td>metal</td>\n",
       "      <td>Come!\\n\\nIf our own lives aren’t directly affe...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>https://www.last.fm/music/kanye%2bwest/_/feedback</td>\n",
       "      <td>Feedback</td>\n",
       "      <td>Kanye West</td>\n",
       "      <td>['aggressive']</td>\n",
       "      <td>1</td>\n",
       "      <td>3.080000</td>\n",
       "      <td>5.870000</td>\n",
       "      <td>5.490000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49fT6owWuknekShh9utsjv</td>\n",
       "      <td>hip-hop</td>\n",
       "      <td>Ayy, y'all heard about the good news?\\nY'all s...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>https://www.last.fm/music/deftones/_/7%2bwords</td>\n",
       "      <td>7 Words</td>\n",
       "      <td>Deftones</td>\n",
       "      <td>['aggressive', 'angry']</td>\n",
       "      <td>10</td>\n",
       "      <td>3.807121</td>\n",
       "      <td>5.473939</td>\n",
       "      <td>4.729091</td>\n",
       "      <td>1a826083-5585-445f-a708-415dc90aa050</td>\n",
       "      <td>6DoXuH326aAYEN8CnlLmhP</td>\n",
       "      <td>nu metal</td>\n",
       "      <td>I'll never be the same, breaking decency\\nDon'...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16792</th>\n",
       "      <td>229432</td>\n",
       "      <td>https://www.last.fm/music/noblegases/_/xenon</td>\n",
       "      <td>Xenon</td>\n",
       "      <td>NobleGases</td>\n",
       "      <td>['noble']</td>\n",
       "      <td>2</td>\n",
       "      <td>6.160000</td>\n",
       "      <td>3.695000</td>\n",
       "      <td>6.130000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1AePjgLLtzF0abbfcgYdLI</td>\n",
       "      <td>chill</td>\n",
       "      <td>You're floating out astray\\nThis cold and life...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16793</th>\n",
       "      <td>229435</td>\n",
       "      <td>https://www.last.fm/music/kurt%2bvile/_/wild%2...</td>\n",
       "      <td>Wild Imagination</td>\n",
       "      <td>Kurt Vile</td>\n",
       "      <td>['transparent']</td>\n",
       "      <td>2</td>\n",
       "      <td>6.925000</td>\n",
       "      <td>4.975000</td>\n",
       "      <td>6.190000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1Gn0oYQiQHp7KF4DcR2g4t</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm looking at you\\nBut It's only a picture so...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16794</th>\n",
       "      <td>229436</td>\n",
       "      <td>https://www.last.fm/music/portugal.%2bthe%2bma...</td>\n",
       "      <td>Oh Lord</td>\n",
       "      <td>Portugal. The Man</td>\n",
       "      <td>['transparent']</td>\n",
       "      <td>1</td>\n",
       "      <td>5.370000</td>\n",
       "      <td>3.450000</td>\n",
       "      <td>5.330000</td>\n",
       "      <td>7ea228f9-16d0-474d-8c51-5a1a9810ddde</td>\n",
       "      <td>6YG8cjbrjhDhlYMiQnibUD</td>\n",
       "      <td>indie</td>\n",
       "      <td>\\n\\n\\nWhere do I fit in\\nI am waiting here for...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16795</th>\n",
       "      <td>229443</td>\n",
       "      <td>https://www.last.fm/music/porcelain%2band%2bth...</td>\n",
       "      <td>Transparent</td>\n",
       "      <td>Porcelain and The Tramps</td>\n",
       "      <td>['transparent']</td>\n",
       "      <td>3</td>\n",
       "      <td>6.613333</td>\n",
       "      <td>4.633333</td>\n",
       "      <td>5.773333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>industrial</td>\n",
       "      <td>Wish I were transparent\\nYou could see right t...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16796</th>\n",
       "      <td>229473</td>\n",
       "      <td>https://www.last.fm/music/daniel%2blanois/_/lo...</td>\n",
       "      <td>Lovechild</td>\n",
       "      <td>Daniel Lanois</td>\n",
       "      <td>['transparent']</td>\n",
       "      <td>2</td>\n",
       "      <td>6.685000</td>\n",
       "      <td>4.405000</td>\n",
       "      <td>5.625000</td>\n",
       "      <td>16c3d394-c4d4-4dc2-bbf1-b2bef3ac861c</td>\n",
       "      <td>4fVObxldDzxxRD6a5Eth9s</td>\n",
       "      <td>indie</td>\n",
       "      <td>I CAUGHT HER STARING, A PSYCHEDELIC DANCER,\\nG...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16797 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                         lastfm_url  \\\n",
       "0               1  https://www.last.fm/music/metallica/_/st.%2banger   \n",
       "1               3  https://www.last.fm/music/m.i.a./_/bamboo%2bbanga   \n",
       "2               5  https://www.last.fm/music/drowning%2bpool/_/st...   \n",
       "3              11  https://www.last.fm/music/kanye%2bwest/_/feedback   \n",
       "4              13     https://www.last.fm/music/deftones/_/7%2bwords   \n",
       "...           ...                                                ...   \n",
       "16792      229432       https://www.last.fm/music/noblegases/_/xenon   \n",
       "16793      229435  https://www.last.fm/music/kurt%2bvile/_/wild%2...   \n",
       "16794      229436  https://www.last.fm/music/portugal.%2bthe%2bma...   \n",
       "16795      229443  https://www.last.fm/music/porcelain%2band%2bth...   \n",
       "16796      229473  https://www.last.fm/music/daniel%2blanois/_/lo...   \n",
       "\n",
       "                  track                    artist  \\\n",
       "0             St. Anger                 Metallica   \n",
       "1          Bamboo Banga                    M.I.A.   \n",
       "2               Step Up             Drowning Pool   \n",
       "3              Feedback                Kanye West   \n",
       "4               7 Words                  Deftones   \n",
       "...                 ...                       ...   \n",
       "16792             Xenon                NobleGases   \n",
       "16793  Wild Imagination                 Kurt Vile   \n",
       "16794           Oh Lord         Portugal. The Man   \n",
       "16795       Transparent  Porcelain and The Tramps   \n",
       "16796         Lovechild             Daniel Lanois   \n",
       "\n",
       "                                            seeds  number_of_emotion_tags  \\\n",
       "0                                  ['aggressive']                       8   \n",
       "1      ['aggressive', 'fun', 'sexy', 'energetic']                      13   \n",
       "2                                  ['aggressive']                       9   \n",
       "3                                  ['aggressive']                       1   \n",
       "4                         ['aggressive', 'angry']                      10   \n",
       "...                                           ...                     ...   \n",
       "16792                                   ['noble']                       2   \n",
       "16793                             ['transparent']                       2   \n",
       "16794                             ['transparent']                       1   \n",
       "16795                             ['transparent']                       3   \n",
       "16796                             ['transparent']                       2   \n",
       "\n",
       "       valence_tags  arousal_tags  dominance_tags  \\\n",
       "0          3.710000      5.833000        5.427250   \n",
       "1          6.555071      5.537214        5.691357   \n",
       "2          2.971389      5.537500        4.726389   \n",
       "3          3.080000      5.870000        5.490000   \n",
       "4          3.807121      5.473939        4.729091   \n",
       "...             ...           ...             ...   \n",
       "16792      6.160000      3.695000        6.130000   \n",
       "16793      6.925000      4.975000        6.190000   \n",
       "16794      5.370000      3.450000        5.330000   \n",
       "16795      6.613333      4.633333        5.773333   \n",
       "16796      6.685000      4.405000        5.625000   \n",
       "\n",
       "                                       mbid              spotify_id  \\\n",
       "0      727a2529-7ee8-4860-aef6-7959884895cb  3fOc9x06lKJBhz435mInlH   \n",
       "1      99dd2c8c-e7c1-413e-8ea4-4497a00ffa18  6tqFC1DIOphJkCwrjVzPmg   \n",
       "2      49e7b4d2-3772-4301-ba25-3cc46ceb342e  4Q1w4Ryyi8KNxxaFlOQClK   \n",
       "3                                       NaN  49fT6owWuknekShh9utsjv   \n",
       "4      1a826083-5585-445f-a708-415dc90aa050  6DoXuH326aAYEN8CnlLmhP   \n",
       "...                                     ...                     ...   \n",
       "16792                                   NaN  1AePjgLLtzF0abbfcgYdLI   \n",
       "16793                                   NaN  1Gn0oYQiQHp7KF4DcR2g4t   \n",
       "16794  7ea228f9-16d0-474d-8c51-5a1a9810ddde  6YG8cjbrjhDhlYMiQnibUD   \n",
       "16795                                   NaN                     NaN   \n",
       "16796  16c3d394-c4d4-4dc2-bbf1-b2bef3ac861c  4fVObxldDzxxRD6a5Eth9s   \n",
       "\n",
       "            genre                                              Lyric language  \n",
       "0           metal  Saint Anger 'round my neck\\nSaint Anger 'round...       en  \n",
       "1         hip-hop  Road runner, road runner\\nGoing hundred mile p...       en  \n",
       "2           metal  Come!\\n\\nIf our own lives aren’t directly affe...       en  \n",
       "3         hip-hop  Ayy, y'all heard about the good news?\\nY'all s...       en  \n",
       "4        nu metal  I'll never be the same, breaking decency\\nDon'...       en  \n",
       "...           ...                                                ...      ...  \n",
       "16792       chill  You're floating out astray\\nThis cold and life...       en  \n",
       "16793         NaN  I'm looking at you\\nBut It's only a picture so...       en  \n",
       "16794       indie  \\n\\n\\nWhere do I fit in\\nI am waiting here for...       en  \n",
       "16795  industrial  Wish I were transparent\\nYou could see right t...       en  \n",
       "16796       indie  I CAUGHT HER STARING, A PSYCHEDELIC DANCER,\\nG...       en  \n",
       "\n",
       "[16797 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('../out.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a754827-5cdc-4eea-aa68-6daa6cc46fad",
   "metadata": {},
   "source": [
    "## Load Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35ff215e-053f-46c7-bd06-def419fba467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40f05ad-864c-40b6-834c-bfd62452fa94",
   "metadata": {},
   "source": [
    "## Preprocess Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb55f0c7-5eeb-4a6f-a59e-b59345014fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['seeds'] = data['seeds'].apply(lambda i: eval(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e499d65a-53a0-466d-9525-a1c38aafd88e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4778234208489611"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['seeds'].apply(len).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c266ae38-c603-49f4-a0f1-14c108269b8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocess Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f25c794a-1de1-45f0-b653-cf8df71183aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_repr(lyrics: str, tags: list) -> tuple[str, str]:\n",
    "    return f'Q: {lyrics}\\nA: {\", \".join(tags)} |EndOfText|\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee76642-9a01-41c6-955d-afe19fbdb62d",
   "metadata": {},
   "source": [
    "This code creates a training file with all song lyrics + labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4ca58722-56fe-470e-b5ac-b8dc5e0ad178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7259c849aa11477a9779ab8bffc6e0bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16797 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompts = data['Lyric']\n",
    "tags = data['seeds']\n",
    "\n",
    "texts = '\\n'.join([text_repr(prompts.iloc[i], tags.iloc[i]) for i in tqdm(range(len(prompts)))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6379d9d1-b4b9-4b8b-944b-5fbf8c51ff75",
   "metadata": {},
   "source": [
    "## Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "13cf8c2e-dfd6-489b-8704-784d36e568f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = tokenizer(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6a0fce0a-ac0f-445a-837b-4dd5fa9da107",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "abaeccfb-d4f9-4bf8-a2d3-e63c251abac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = {\n",
    "    f'chunk {i // chunk_size}': tokenized_data['input_ids'][i:i + chunk_size]\n",
    "    for i in range(0, len(tokenized_data['input_ids']), chunk_size)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1882b74c-305b-40ed-a4c2-57ba0ecc2956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5986757,)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tokenized_data['input_ids']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32913fbc-2952-4968-b776-67def73ed7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de9d7c984c514c85a62101bcb8aac284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chunks = {\n",
    "    k: [t[i : i + chunk_size] for i in range(0, len(tokenized_data), chunk_size)]\n",
    "    for k, t in tqdm(tokenized_data.items())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0eb51b0a-d18b-4852-badd-48baabf0fbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(chunks[list(chunks.keys())[-1]]) < chunk_size:\n",
    "    del chunks[list(chunks.keys())[-1]]  # Drop last chunk if it's smaller than chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "55ce43aa-6bc3-408a-bdf9-3f4ce2a732b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val = train_test_split(list(chunks.values()), test_size=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1e6b9c09-1e70-429c-9aba-97b8d9a3d4ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42093, 128)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0cf561ee-cfcf-4558-bbc2-0d35121652f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3f6f5bf6-c0b0-410d-a193-91d8e706742f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53463' max='78930' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [53463/78930 21:48:14 < 10:23:12, 0.68 it/s, Epoch 10.16/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.006400</td>\n",
       "      <td>3.010101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.951400</td>\n",
       "      <td>2.995259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.910200</td>\n",
       "      <td>2.992666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.882900</td>\n",
       "      <td>2.980284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.892700</td>\n",
       "      <td>2.972169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.845200</td>\n",
       "      <td>2.967006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.849600</td>\n",
       "      <td>2.962205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.827600</td>\n",
       "      <td>2.959093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.812100</td>\n",
       "      <td>2.952968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.800300</td>\n",
       "      <td>2.954942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[138], line 21\u001b[0m\n\u001b[1;32m      2\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      3\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilgpt2-finetuned-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     save_total_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     14\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     15\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m     19\u001b[0m )\n\u001b[0;32m---> 21\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.10/site-packages/transformers/trainer.py:1633\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1630\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1631\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1632\u001b[0m )\n\u001b[0;32m-> 1633\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1638\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.10/site-packages/transformers/trainer.py:1902\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1900\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1901\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1902\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1904\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1905\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1906\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1907\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1908\u001b[0m ):\n\u001b[1;32m   1909\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1910\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.10/site-packages/transformers/trainer.py:2645\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2642\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2644\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2645\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2648\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.10/site-packages/transformers/trainer.py:2677\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2675\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2676\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2677\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2678\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2679\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2680\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1075\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1075\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1090\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:899\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    889\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    890\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    891\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    896\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    897\u001b[0m     )\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 899\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    910\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:426\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    424\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    425\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(hidden_states)\n\u001b[0;32m--> 426\u001b[0m feed_forward_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n\u001b[1;32m    428\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m feed_forward_hidden_states\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:356\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    354\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(hidden_states)\n\u001b[1;32m    355\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(hidden_states)\n\u001b[0;32m--> 356\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.10/site-packages/torch/nn/functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(p))\n\u001b[0;32m-> 1252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"distilgpt2-finetuned-v{ver}\",\n",
    "    save_total_limit=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # eval_steps=10,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=15\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=x_train,\n",
    "    eval_dataset=x_val,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4a0de6d9-cf0c-4ab4-ad64-4127ec1910fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "text_generator = pipeline('text-generation', tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "9512b8f8-d09b-48b3-8771-279936d788b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "LYRICS = \"\"\"\n",
    "Fuck it all and no regrets\n",
    "I hit the lights on these dark sets\n",
    "I need a voice to let myself\n",
    "To let myself go free\n",
    "Fuck it all and fuckin' no regrets\n",
    "I hit the lights on these dark sets\n",
    "Medallion noose, I hang myself\n",
    "Saint Anger 'round my neck\n",
    "\n",
    "I feel my world shake\n",
    "Like an earthquake\n",
    "Hard to see clear\n",
    "Is it me? Is it fear?\n",
    "I'm madly in anger with you (x4)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "05caa612-19ca-4489-946e-846ea4e560e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_lyrics = 'Q: ' + LYRICS + '\\nA: '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "f48245de-7592-4833-a260-02bd3074b9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: \n",
      "Fuck it all and no regrets\n",
      "I hit the lights on these dark sets\n",
      "I need a voice to let myself\n",
      "To let myself go free\n",
      "Fuck it all and fuckin' no regrets\n",
      "I hit the lights on these dark sets\n",
      "Medallion noose, I hang myself\n",
      "Saint Anger 'round my neck\n",
      "\n",
      "I feel my world shake\n",
      "Like an earthquake\n",
      "Hard to see clear\n",
      "Is it me? Is it fear?\n",
      "I'm madly in anger with you (x4)\n",
      "\n",
      "A: Get down, brooding |EndOfText|\n"
     ]
    }
   ],
   "source": [
    "print(text_generator(processed_lyrics, max_length=len(tokenizer(processed_lyrics)['input_ids']) + 10)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "fcad254e-9728-457a-ab38-3b4dff05c193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saint Anger 'round my neck\n",
      "Saint Anger 'round my neck\n",
      "She never gets respect\n",
      "Saint Anger 'round my neck\n",
      "(You flush it out, you flush it out)\n",
      "Saint Anger 'round my neck\n",
      "(You flush it out, you flush it out)\n",
      "He never gets respect\n",
      "(You flush it out, you flush it out)\n",
      "Saint Anger 'round my neck\n",
      "(You flush it out, you flush it out)\n",
      "She never gets respect\n",
      "\n",
      "Fuck it all and no regrets\n",
      "I hit the lights on these dark sets\n",
      "I need a voice to let myself\n",
      "To let myself go free\n",
      "Fuck it all and fuckin' no regrets\n",
      "I hit the lights on these dark sets\n",
      "Medallion noose, I hang myself\n",
      "Saint Anger 'round my neck\n",
      "\n",
      "I feel my world shake\n",
      "Like an earthquake\n",
      "Hard to see clear\n",
      "Is it me? Is it fear?\n",
      "I'm madly in anger with you (x4)\n",
      "\n",
      "Saint Anger 'round my neck\n",
      "Saint Anger 'round my neck\n",
      "She never gets respect\n",
      "Saint Anger 'round my neck\n",
      "(You flush it out, you flush it out)\n",
      "Saint Anger 'round my neck\n",
      "(You flush it out, you flush it out)\n",
      "She never gets respect\n",
      "(You flush it out, you flush it out)\n",
      "Saint Anger 'round my neck\n",
      "(You flush it out, you flush it out)\n",
      "She never gets respect\n",
      "\n",
      "Fuck it all and no regrets\n",
      "I hit the lights on these dark sets\n",
      "I need a voice to let myself\n",
      "To let myself go free\n",
      "Fuck it all and fuckin' no regrets\n",
      "I hit the lights on these dark sets\n",
      "Medallion noose, I hang myself\n",
      "Saint Anger 'round my neck\n",
      "\n",
      "I feel my world shake\n",
      "Like an earthquake\n",
      "Hard to see clear\n",
      "Is it me? Is it fear?\n",
      "I'm madly in anger with you (x4)\n",
      "\n",
      "and I want my anger to be healthy\n",
      "and I want my anger just for me\n",
      "and I need my anger not to control\n",
      "and I want my anger to be me\n",
      "and I need to set my anger free\n",
      "and I need to set my anger free\n",
      "and I need to set my anger free\n",
      "and I need to set my anger free\n",
      "SET IT FREE\n",
      "\n",
      "Fuck it all and no regrets\n",
      "I hit the lights on these dark sets\n",
      "I need a voice to let myself\n",
      "To let myself go free\n",
      "Fuck it all and fuckin' no regrets\n",
      "I hit the lights on these dark sets\n",
      "Medallion noose, I hang myself\n",
      "Saint Anger 'round my neck\n",
      "\n",
      "I feel my world shake\n",
      "Like an earthquake\n",
      "Hard to see clear\n",
      "Is it me? Is it fear?\n",
      "I'm madly in anger with you (x4)\n"
     ]
    }
   ],
   "source": [
    "print(data['Lyric'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f71bfbb7-0ccd-4498-add8-53eb0564917d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saint Anger 'round my neck\n",
      "Saint Anger 'round my neck\n",
      "She never gets respect\n",
      "Saint Anger 'round my neck\n",
      "(You flush it out, you flush it out)\n",
      "Saint Anger 'round my neck\n",
      "(You flush it out, you flush it out)\n",
      "He never gets respect\n",
      "(You flush it out, you flush it out)\n",
      "Saint Anger 'round my neck\n",
      "(You flush it out, you flush it out)\n",
      "She never gets respect\n",
      "\n",
      "Fuck it all and no regrets\n",
      "I hit the lights on these dark sets\n",
      "I need a voice to let myself\n",
      "To let myself go free\n",
      "Fuck it all and fuckin' no regrets\n",
      "I hit the lights on these dark sets\n",
      "Medallion noose, I hang myself\n",
      "Saint Anger 'round my neck\n",
      "\n",
      "I feel my world shake\n",
      "Like an earthquake\n",
      "Hard to see clear\n",
      "Is it me? Is it fear?\n",
      "I'm madly in anger with you (x4)\n",
      "\n",
      "Saint Anger 'round my neck\n",
      "Saint Anger 'round my neck\n",
      "She never gets respect\n",
      "Saint Anger 'round my neck\n",
      "(You flush it out, you flush it out)\n",
      "Saint Anger 'round my neck\n",
      "(You flush it out, you flush it out)\n",
      "She never gets respect\n",
      "(You flush it out, you flush it out)\n",
      "Saint Anger 'round my neck\n",
      "(You flush it out, you flush it out)\n",
      "She never gets respect\n",
      "\n",
      "Fuck it all and no regrets\n",
      "I hit the lights on these dark sets\n",
      "I need a voice to let myself\n",
      "To let myself go free\n",
      "Fuck it all and fuckin' no regrets\n",
      "I hit the lights on these dark sets\n",
      "Medallion noose, I hang myself\n",
      "Saint Anger 'round my neck\n",
      "\n",
      "I feel my world shake\n",
      "Like an earthquake\n",
      "Hard to see clear\n",
      "Is it me? Is it fear?\n",
      "I'm madly in anger with you (x4)\n",
      "\n",
      "and I want my anger to be healthy\n",
      "and I want my anger just for me\n",
      "and I need my anger not to control\n",
      "and I want my anger to be me\n",
      "and I need to set my anger free\n",
      "and I need to set my anger free\n",
      "and I need to set my anger free\n",
      "and I need to set my anger free\n",
      "SET IT FREE\n",
      "\n",
      "Fuck it all and no regrets\n",
      "I hit the lights on these dark sets\n",
      "I need a voice to let myself\n",
      "To let myself go free\n",
      "Fuck it all and fuckin' no regrets\n",
      "I hit the lights on these dark sets\n",
      "Medallion noose, I hang myself\n",
      "Saint Anger 'round my neck\n",
      "\n",
      "I feel my world shake\n",
      "Like an earthquake\n",
      "Hard to see clear\n",
      "Is it me? Is it fear?\n",
      "I'm madly in anger with you (x4)\n"
     ]
    }
   ],
   "source": [
    "print(data['Lyric'].iloc[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
